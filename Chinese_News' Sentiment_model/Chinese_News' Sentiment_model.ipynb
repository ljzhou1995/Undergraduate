{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting..Please wait.\n",
      "Finish.\n",
      "Segmenting..Please wait.\n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "# python3.6\n",
    "# windows7 & pycharm\n",
    "\n",
    "# original author: https://github.com/sunxiangguo/chinese_text_classification\n",
    "# improve: ljzhou\n",
    "\n",
    "# target: Chinese text(financial news) segment\n",
    "\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "from Tools import savefile, readfile\n",
    "\n",
    "\n",
    "# corpus_path is the unsegmented corpus path\n",
    "# seg_path is the segmented corpus path\n",
    "def corpus_segment(corpus_path, seg_path):   \n",
    "    catelist = os.listdir(corpus_path)  # Gets all subdirectories under corpus_path\n",
    "    ## In fact, the name of subdirectories is the category, and you can use more subdirectories in your object\n",
    "    print(\"Segmenting..Please wait.\")\n",
    "    \n",
    "    # Gets all the files under each directory (category)\n",
    "    for mydir in catelist:\n",
    "        class_path = corpus_path + mydir + \"/\"\n",
    "        seg_dir = seg_path + mydir + \"/\"\n",
    "        if not os.path.exists(seg_dir):  # Whether there is a word segmentation directory, if not, create it\n",
    "            os.makedirs(seg_dir)\n",
    "        file_list = os.listdir(class_path)  # Get all the text in a category in an unsegmented term repository\n",
    "            \n",
    "    # Traverse all files in the category directory and to process\n",
    "        for file_path in file_list:\n",
    "            fullname = class_path + file_path\n",
    "            content = readfile(fullname)\n",
    "            content = content.replace('\\r\\n'.encode('utf-8'), ''.encode('utf-8')).strip()  # Delete line breaks\n",
    "            content = content.replace(' '.encode('utf-8'), ''.encode('utf-8')).strip()  # Delete empty lines, extra spaces\n",
    "            content_seg = jieba.cut(content)  # segment\n",
    "            savefile(seg_dir + file_path, ' '.join(content_seg).encode('utf-8'))  # Save the segmented file\n",
    "    print(\"Finish.\")\n",
    "\n",
    "    # Segmentation of train and test sets.\n",
    "if __name__ == \"__main__\":\n",
    "    corpus_path = \"./train_corpus/\"  # Unsegmented\n",
    "    seg_path = \"./train_corpus_seg/\"  # Segmented\n",
    "    corpus_segment(corpus_path, seg_path)\n",
    "    \n",
    "    corpus_path = \"./test_corpus/\"  # Unsegmented\n",
    "    seg_path = \"./test_corpus_seg/\"  # Segmented\n",
    "    corpus_segment(corpus_path, seg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bunch Finish\n",
      "Bunch Finish\n"
     ]
    }
   ],
   "source": [
    "# target: Create a Bunch\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets.base import Bunch\n",
    "from Tools import readfile\n",
    "\n",
    "    # Create a Bunch\n",
    "def corpus2Bunch(wordbag_path, seg_path):\n",
    "    catelist = os.listdir(seg_path)\n",
    "    bunch = Bunch(target_name=[], label=[], filenames=[], contents=[])\n",
    "    bunch.target_name.extend(catelist)    # Expand the original list with the new list (addlist)\n",
    "    \n",
    "    # Add an element to the original list\n",
    "    for mydir in catelist:\n",
    "        class_path = seg_path + mydir + \"/\"\n",
    "        file_list = os.listdir(class_path)\n",
    "        for file_path in file_list:\n",
    "            fullname = class_path + file_path\n",
    "            bunch.label.append(mydir)\n",
    "            bunch.filenames.append(fullname)\n",
    "            bunch.contents.append(readfile(fullname))\n",
    "\n",
    "    # Store bunch in wordbag_path\n",
    "    with open(wordbag_path, \"wb\") as file_obj:\n",
    "        pickle.dump(bunch, file_obj)\n",
    "    print(\"Bunch Finish\")\n",
    "\n",
    "    # Bunch of train and test set.\n",
    "if __name__ == \"__main__\":\n",
    "    wordbag_path = \"train_word_bag/train_set.dat\"  # Bunch path\n",
    "    seg_path = \"train_corpus_seg/\"  # segmented corpus path\n",
    "    corpus2Bunch(wordbag_path, seg_path)\n",
    "\n",
    "    wordbag_path = \"test_word_bag/test_set.dat\"  # Bunch path\n",
    "    seg_path = \"test_corpus_seg/\"  # segmented corpus path\n",
    "    corpus2Bunch(wordbag_path, seg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF finish\n",
      "TF-IDF finish\n"
     ]
    }
   ],
   "source": [
    "# target: Create a word bag\n",
    "\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from Tools import readfile, readbunchobj, writebunchobj\n",
    "\n",
    "    # Remove stop words\n",
    "def vector_space(stopword_path, bunch_path, space_path, train_tfidf_path=None):\n",
    "    stpwrdlst = readfile(stopword_path).splitlines()    # stop words file read\n",
    "    bunch = readbunchobj(bunch_path)\n",
    "    tfidfspace = Bunch(target_name=bunch.target_name, label=bunch.label, filenames=bunch.filenames, tdm=[],\n",
    "                       vocabulary={})\n",
    "    \n",
    "    # Build a word bag by using TF-IDF\n",
    "    if train_tfidf_path is not None:\n",
    "        trainbunch = readbunchobj(train_tfidf_path)\n",
    "        tfidfspace.vocabulary = trainbunch.vocabulary\n",
    "        vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5,\n",
    "                                     vocabulary=trainbunch.vocabulary)\n",
    "        tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5)\n",
    "        tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "        tfidfspace.vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "    writebunchobj(space_path, tfidfspace)\n",
    "    print(\"TF-IDF finish\")\n",
    "\n",
    "    # word bag of train and test set\n",
    "if __name__ == '__main__':\n",
    "    stopword_path = \"train_word_bag/hlt_stop_words.txt\"    # Stop word file\n",
    "    bunch_path = \"train_word_bag/train_set.dat\"    # Bunch path\n",
    "    space_path = \"train_word_bag/tfdifspace.dat\"    # TF-IDF path\n",
    "    vector_space(stopword_path, bunch_path, space_path)\n",
    "\n",
    "    bunch_path = \"test_word_bag/test_set.dat\"    # Bunch path\n",
    "    space_path = \"test_word_bag/testspace.dat\"    # Word bag path\n",
    "    train_tfidf_path = \"train_word_bag/tfdifspace.dat\"    # TF-IDF path\n",
    "    vector_space(stopword_path, bunch_path, space_path, train_tfidf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_corpus_seg/Neg_news/11.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/24.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/25.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/3.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/41.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/48.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/51.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/6.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/60.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/62.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/64.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/65.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/69.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/72.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/74.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/75.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/86.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/9.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Neg_news/96.txt : Correct category: Neg_news  -->Prediction: Pos_news\n",
      "test_corpus_seg/Pos_news/1.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/113.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/115.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/120.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/17.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/20.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/22.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/24.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/28.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/33.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/39.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/41.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/43.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/44.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/45.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/56.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/60.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/61.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/64.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/65.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/72.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/77.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/79.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "test_corpus_seg/Pos_news/83.txt : Correct category: Pos_news  -->Prediction: Neg_news\n",
      "B_NB:\n",
      "Precise:0.809\n",
      "Recall:0.807\n",
      "F1-Score:0.807\n",
      "\n",
      "SVC:\n",
      "Precise:0.835\n",
      "Recall:0.834\n",
      "F1-Score:0.834\n",
      "\n",
      "Tree:\n",
      "Precise:0.682\n",
      "Recall:0.677\n",
      "F1-Score:0.678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# target: caculate the sentiments of news and take model evaluation\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from Tools import readbunchobj\n",
    "import csv\n",
    "\n",
    "    # train set\n",
    "trainpath = \"train_word_bag/tfdifspace.dat\"\n",
    "train_set = readbunchobj(trainpath)\n",
    "\n",
    "    # test set\n",
    "testpath = \"test_word_bag/testspace.dat\"\n",
    "test_set = readbunchobj(testpath)\n",
    "\n",
    "    # Training classifier: input word bag vector and classification label\n",
    "B_NB = BernoulliNB(alpha=0.001).fit(train_set.tdm, train_set.label)\n",
    "SVC = svm.SVC(C=0.8,kernel='sigmoid',gamma=10,decision_function_shape='ovo',probability=True).fit(train_set.tdm, train_set.label)\n",
    "Tree = tree.DecisionTreeClassifier(min_samples_leaf=30,min_samples_split=10).fit(train_set.tdm, train_set.label)\n",
    "\n",
    "    # Predict classification results\n",
    "pre_B_NB = B_NB.predict(test_set.tdm)\n",
    "pre_SVC = SVC.predict(test_set.tdm)\n",
    "pre_Tree = Tree.predict(test_set.tdm)\n",
    "\n",
    "\n",
    "    # If you want to check the wrong prediction, you can use this.\n",
    "for flabel, file_name, expct_cate in zip(test_set.label, test_set.filenames, pre_B_NB):    # Please change the pre_B_NB\n",
    "    if flabel != expct_cate:\n",
    "        print(file_name, \": Correct category:\", flabel, \" -->Prediction:\", expct_cate)\n",
    "\n",
    "\n",
    "    # Calculate classification accuracy\n",
    "def metrics_result(actual, predict):\n",
    "    print('Precise:{0:.3f}'.format(metrics.precision_score(actual, predict, average='weighted')))\n",
    "    print('Recall:{0:0.3f}'.format(metrics.recall_score(actual, predict, average='weighted')))\n",
    "    print('F1-Score:{0:.3f}'.format(metrics.f1_score(actual, predict, average='weighted')))\n",
    "    print()\n",
    "    \n",
    "    # evaluation\n",
    "print('B_NB:')\n",
    "metrics_result(test_set.label, pre_B_NB)\n",
    "print('SVC:')\n",
    "metrics_result(test_set.label, pre_SVC)\n",
    "print('Tree:')\n",
    "metrics_result(test_set.label, pre_Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
